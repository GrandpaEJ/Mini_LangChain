{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Mini LangChain","text":"<p>The high-performance LLM orchestration framework. Built in Rust. Optimized for Python and Node.js.</p> <p>Mini LangChain is engineered for developers who need throughput, low latency, and cost-efficiency. By moving orchestration and RAG logic to a memory-safe Rust core, we eliminate the overhead of traditional interpreted frameworks.</p>"},{"location":"#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li>\u26a1 Blazing Fast: Core logic in Rust for sub-millisecond orchestration overhead.</li> <li>\ud83d\udcb0 Cost-Optimizer: Automatic whitespace minification and aggressive prompt caching.</li> <li>\ud83d\udd17 Unified SDKs: Identical API patterns across Python and Node.js.</li> <li>\ud83d\udee0\ufe0f Production Ready: Support for OpenAI, Anthropic, Gemini, SambaNova, and Ollama.</li> </ul>"},{"location":"#quick-start","title":"\ud83d\udee0\ufe0f Quick Start","text":"PythonNode.js <p><pre><code>pip install mini-langchain\n</code></pre> <pre><code>from mini_langchain import SambaNovaLLM, Chain, PromptTemplate\n\ntmpl = PromptTemplate(\"Explain {topic} in 5 words.\")\nllm = SambaNovaLLM(model=\"Meta-Llama-3.1-8B-Instruct\")\nchain = Chain(tmpl, llm)\n\nprint(chain.invoke({\"topic\": \"Rust\"}))\n</code></pre></p> <p><pre><code>npm install mini-langchain\n</code></pre> <pre><code>const { Chain, PromptTemplate, OpenAILLM } = require('mini-langchain');\n\nconst chain = new Chain(\n  new PromptTemplate(\"Hello {name}!\"),\n  new OpenAILLM(\"key\", \"gpt-4\")\n);\n\nconsole.log(await chain.invoke({ name: \"Alice\" }));\n</code></pre></p>"},{"location":"#performance","title":"\ud83d\udcca Performance","text":"<p>Mini LangChain is designed to handle thousands of tokens per second with minimal CPU footprint. Check our Benchmarks to see it in action.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to Mini LangChain will be documented in this file.</p>"},{"location":"changelog/#010-2026-02-01","title":"[0.1.0] - 2026-02-01","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Multi-Provider LLM Support: Added native Rust implementations and bindings for:</li> <li>OpenAI (Standard and OpenAI-compatible like OpenRouter)</li> <li>Anthropic Claude</li> <li>Google Gemini</li> <li>Ollama (Local)</li> <li>SambaNova</li> <li>Memory &amp; Persistence: </li> <li><code>ConversationBufferMemory</code> for chat history management.</li> <li><code>InMemoryCache</code> for prompt/response caching.</li> <li>RAG Implementation:</li> <li><code>Document</code> and <code>TextLoader</code> for data ingestion.</li> <li><code>InMemoryVectorStore</code> with Cosine Similarity.</li> <li><code>Embeddings</code> trait and <code>MockEmbeddings</code> (Transitioning to real providers).</li> <li>Agent Framework: </li> <li><code>AgentExecutor</code> for tool-calling logic.</li> <li><code>Tool</code> trait for custom tool implementation.</li> <li>Cross-Language Bindings:</li> <li>Python: Universal ABI3 wheels support (Python 3.9+), <code>maturin</code> integration.</li> <li>Node.js: Type-safe <code>napi-rs</code> bindings with support for multiple LLM inputs via <code>Either</code>.</li> <li>CI/CD Workflows:</li> <li>Automated publishing to crates.io, NPM, and PyPI.</li> <li>Manual dispatch support for release workflows.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Dependency Upgrades:</li> <li>Upgraded PyO3 to v0.27.2 (Modern Bound API refactor).</li> <li>Upgraded Napi to v3.8.2.</li> <li>Python Packaging: Moved from manual <code>.so</code> linking to standard <code>pyproject.toml</code> and <code>maturin develop</code> workflow.</li> <li>Node.js Type Safety: Refactored <code>Chain</code> and <code>Agent</code> constructors to use <code>Either</code> for robust LLM type handling.</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Resolved PyO3 compilation errors for Python 3.14 by enabling ABI3 support.</li> <li>Fixed casing inconsistencies in Node.js class names (<code>SambaNovaLLM</code> instead of <code>SambaNovaLlm</code>).</li> <li>Removed unused imports and addressed Rust compiler warnings across <code>core</code>, <code>python</code>, and <code>node</code> modules.</li> <li>Fixed GitHub Actions release job skipping by allowing manual workflow dispatch.</li> </ul>"},{"location":"changelog/#testing","title":"Testing","text":"<ul> <li>Comprehensive Python test suite (<code>tests/python/test_providers.py</code>).</li> <li>Comprehensive Node.js test suite (<code>tests/node/test_providers.js</code>).</li> <li>Rust unit tests for prompt formatting and provider serialization.</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome contributions to Mini_LangChain! Whether it's adding a new LLM provider, fixing a bug, or improving documentation.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisite","title":"Prerequisite","text":"<ul> <li>Rust (latest stable)</li> <li>Node.js (v18+)</li> <li>Python (3.9+)</li> </ul>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<ul> <li><code>/core</code>: The Rust source code.</li> <li><code>/python</code>: PyO3 bindings.</li> <li><code>/node</code>: Napi-rs bindings.</li> <li><code>/docs</code>: Documentation source.</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<p>Each layer has its own test suite.</p>"},{"location":"contributing/#rust-core","title":"Rust Core","text":"<pre><code>cd core\ncargo test\n</code></pre>"},{"location":"contributing/#python-sdk","title":"Python SDK","text":"<pre><code># Requires maturin\nmaturin develop\npytest tests/python\n</code></pre>"},{"location":"contributing/#nodejs-sdk","title":"Node.js SDK","text":"<pre><code>cd node\nnpm install\nnpm run build\nnpm test\n</code></pre>"},{"location":"contributing/#cicd-releases","title":"CI/CD &amp; Releases","text":""},{"location":"contributing/#npm-nodejs","title":"NPM (Node.js)","text":"<p>To publish to NPM from GitHub Actions (non-interactively), you must use an Automation Token: 1. Log in to npmjs.com. 2. Go to Access Tokens -&gt; Generate New Token. 3. Select \"Automation\" as the type. This is crucial as it bypasses 2FA challenges during <code>npm publish</code>. 4. Copy the token and add it to your GitHub Repository Secrets as <code>NPM_SECRET_TOKEN</code>.</p>"},{"location":"contributing/#pypi-python","title":"PyPI (Python)","text":"<p>Automated releases to PyPI use a standard API Token: 1. Generate an API token on pypi.org. 2. Add it to GitHub Secrets as <code>PYPI_API_TOKEN</code>.</p>"},{"location":"contributing/#creating-a-pull-request","title":"Creating a Pull Request","text":"<ol> <li>Fork the repo.</li> <li>Create a feature branch.</li> <li>Commit your changes.</li> <li>Open a PR with a clear description of the \"why\" and \"how\".</li> </ol>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>Tracking cost and latency improvements version-wise.</p>"},{"location":"benchmarks/#v010-current","title":"v0.1.0 (Current)","text":"<p>| Metric | Value | Notes | |Str|Val|Notes| |---|---|---| | Latency (Rust) | &lt; 1ms | Overhead added by Rust wrapper | | Token Savings | ~15% | Avg savings via whitespace minification | | Cache Lookup | &lt; 0.1ms | In-Memory Hash Map |</p>"},{"location":"node/api/","title":"Node.js API Reference","text":"<p>The Node.js SDK provides high-performance bindings to the Rust core using Napi-rs.</p>"},{"location":"node/api/#core-classes","title":"Core Classes","text":""},{"location":"node/api/#chain","title":"<code>Chain</code>","text":"<p>Orchestrates prompts and LLMs. <pre><code>const { Chain, PromptTemplate, OpenAILLM } = require('mini-langchain');\n\nconst prompt = new PromptTemplate(\"What is {topic}?\");\nconst llm = new OpenAILLM(\"api-key\", \"gpt-4\");\nconst chain = new Chain(prompt, llm);\n\nconst result = await chain.invoke({ topic: \"Rust\" });\n</code></pre></p>"},{"location":"node/api/#prompttemplate","title":"<code>PromptTemplate</code>","text":"<pre><code>const template = new PromptTemplate(\"Hello {name}!\");\n</code></pre>"},{"location":"node/api/#inmemoryvectorstore","title":"<code>InMemoryVectorStore</code>","text":"<p>High-speed vector storage in the Rust layer. <pre><code>const { Document, InMemoryVectorStore, MockEmbeddings } = require('mini-langchain');\n\nconst store = new InMemoryVectorStore(new MockEmbeddings());\nawait store.addDocuments([new Document(\"Data point\")]);\n</code></pre></p>"},{"location":"node/api/#llm-providers","title":"LLM Providers","text":"<ul> <li><code>OpenAILLM</code></li> <li><code>AnthropicLLM</code></li> <li><code>SambaNovaLLM</code></li> <li><code>GoogleGenAILLM</code></li> <li><code>OllamaLLM</code></li> </ul>"},{"location":"node/examples/","title":"Node.js Examples","text":"<p>Learn how to use Mini_LangChain in your Node.js applications.</p>"},{"location":"node/examples/#simple-completion","title":"Simple Completion","text":"<pre><code>const { Chain, PromptTemplate, SambaNovaLLM } = require('mini-langchain');\n\nasync function main() {\n  const llm = new SambaNovaLLM(\"llama3-70b\");\n  const prompt = new PromptTemplate(\"Explain {topic} in 5 words.\");\n  const chain = new Chain(prompt, llm);\n\n  const res = await chain.invoke({ topic: \"Async functions\" });\n  console.log(res);\n}\n\nmain();\n</code></pre>"},{"location":"node/examples/#rag-workflow","title":"RAG Workflow","text":"<pre><code>const { \n  TextLoader, \n  InMemoryVectorStore, \n  MockEmbeddings, \n  Chain, \n  PromptTemplate,\n  OpenAILLM \n} = require('mini-langchain');\n\nasync function rag() {\n  // 1. Load data\n  const loader = new TextLoader(\"./knowledge.txt\");\n  const docs = loader.load();\n\n  // 2. Index in Rust\n  const store = new InMemoryVectorStore(new MockEmbeddings());\n  await store.addDocuments(docs);\n\n  // 3. Search and Answer\n  const retrieved = await store.similaritySearch(\"specific topic\", 1);\n  const context = retrieved[0].pageContent;\n\n  const chain = new Chain(\n    new PromptTemplate(\"Context: {context}\\n\\nQuestion: {query}\"),\n    new OpenAILLM(\"key\", \"gpt-4\")\n  );\n\n  console.log(await chain.invoke({ context, query: \"summary\" }));\n}\n</code></pre>"},{"location":"node/getting_started/","title":"Getting Started with Node.js","text":""},{"location":"node/getting_started/#installation","title":"Installation","text":"<pre><code>npm install mini-langchain-node\n</code></pre>"},{"location":"node/getting_started/#basic-usage","title":"Basic Usage","text":""},{"location":"node/getting_started/#1-templates","title":"1. Templates","text":"<pre><code>const { PromptTemplate } = require('mini-langchain-node');\n\n// Whitespace is automatically minified\nconst tmpl = new PromptTemplate(\"  Hello {name}   \", [\"name\"]);\n</code></pre>"},{"location":"node/getting_started/#2-connect-to-llm","title":"2. Connect to LLM","text":"<pre><code>const { SambaNovaLlm } = require('mini-langchain-node');\n\nconst llm = new SambaNovaLlm(\n    \"Meta-Llama-3.1-8B-Instruct\",\n    process.env.SAMBANOVA_API_KEY, \n    \"You are a helpful assistant.\" // System Prompt\n);\n</code></pre>"},{"location":"node/getting_started/#3-chains","title":"3. Chains","text":"<pre><code>const { Chain } = require('mini-langchain-node');\n\nconst chain = new Chain(tmpl, llm);\n\n(async () =&gt; {\n    const res = await chain.invoke({ \"name\": \"Node User\" });\n    console.log(res);\n})();\n</code></pre>"},{"location":"python/api/","title":"Python API Reference","text":"<p>Mini_LangChain's Python SDK is built on a high-performance Rust core, exposed via PyO3.</p>"},{"location":"python/api/#core-components","title":"Core Components","text":""},{"location":"python/api/#chain","title":"<code>Chain</code>","text":"<p>The central orchestration unit. <pre><code>from mini_langchain import Chain, PromptTemplate, OpenAILLM\n\nprompt = PromptTemplate(\"What is {topic}?\")\nllm = OpenAILLM(api_key=\"...\", model=\"gpt-4o\")\nchain = Chain(prompt, llm)\n\nresult = chain.invoke({\"topic\": \"Rust\"})\n</code></pre></p>"},{"location":"python/api/#prompttemplate","title":"<code>PromptTemplate</code>","text":"<p>Handles input orchestration and variable injection. <pre><code>template = PromptTemplate(\"Translate {text} to {language}\")\n</code></pre></p>"},{"location":"python/api/#document-vectorstore","title":"<code>Document</code> &amp; <code>VectorStore</code>","text":"<p>For RAG workflows. <pre><code>from mini_langchain import Document, InMemoryVectorStore, MockEmbeddings\n\ndoc = Document(page_content=\"Rust is fast.\")\nvectorstore = InMemoryVectorStore(MockEmbeddings())\nvectorstore.add_documents([doc])\n</code></pre></p>"},{"location":"python/api/#llm-providers","title":"LLM Providers","text":"<ul> <li><code>OpenAILLM</code></li> <li><code>AnthropicLLM</code></li> <li><code>SambaNovaLLM</code> (Optimized for tokens/sec)</li> <li><code>GoogleGenAILLM</code></li> <li><code>OllamaLLM</code> (Local inference)</li> </ul>"},{"location":"python/examples/","title":"Python Examples","text":"<p>Here are some common patterns for using Mini_LangChain in Python.</p>"},{"location":"python/examples/#basic-llm-chain","title":"Basic LLM Chain","text":"<pre><code>from mini_langchain import Chain, PromptTemplate, SambaNovaLLM\n\n# Fast inference with SambaNova\nllm = SambaNovaLLM(model=\"llama3-70b\")\nprompt = PromptTemplate(\"Explain {concept} in one sentence.\")\nchain = Chain(prompt, llm)\n\nprint(chain.invoke({\"concept\": \"Recursion\"}))\n</code></pre>"},{"location":"python/examples/#simple-rag-retrieval-augmented-generation","title":"Simple RAG (Retrieval Augmented Generation)","text":"<pre><code>import asyncio\nfrom mini_langchain import Document, InMemoryVectorStore, MockEmbeddings\n\nasync def run_rag():\n    # 1. Create Documents\n    docs = [\n        Document(page_content=\"Mini_LangChain is built in Rust.\"),\n        Document(page_content=\"It supports Python and Node.js.\")\n    ]\n\n    # 2. Setup Vector Store\n    embeddings = MockEmbeddings()\n    vectorstore = InMemoryVectorStore(embeddings)\n\n    # 3. Add and Search\n    await vectorstore.add_documents(docs)\n    results = await vectorstore.similarity_search(\"What is it built in?\", k=1)\n\n    for doc in results:\n        print(f\"Result: {doc.page_content}\")\n\nasyncio.run(run_rag())\n</code></pre>"},{"location":"python/examples/#conversation-with-memory","title":"Conversation with Memory","text":"<pre><code>from mini_langchain import Chain, PromptTemplate, OpenAILLM, ConversationBufferMemory\n\nmemory = ConversationBufferMemory()\nllm = OpenAILLM(api_key=\"...\", model=\"gpt-3.5-turbo\")\nprompt = PromptTemplate(\"Previous: {history}\\nUser: {input}\")\n\nchain = Chain(prompt, llm, memory=memory)\n\n# First interaction\nchain.invoke({\"input\": \"My name is Alice.\"})\n\n# Second interaction (remembers name)\nprint(chain.invoke({\"input\": \"What is my name?\"}))\n</code></pre>"},{"location":"python/getting_started/","title":"Getting Started with Python","text":""},{"location":"python/getting_started/#installation","title":"Installation","text":"<p>Ensure you have the shared library (<code>mini_langchain.so</code>) available in your python path.</p>"},{"location":"python/getting_started/#basic-usage","title":"Basic Usage","text":""},{"location":"python/getting_started/#1-templates","title":"1. Templates","text":"<pre><code>from mini_langchain import PromptTemplate\n\n# Whitespace is automatically minified to save tokens!\ntmpl = PromptTemplate(\"  Hello {name}   \", [\"name\"])\n</code></pre>"},{"location":"python/getting_started/#2-connect-to-llm","title":"2. Connect to LLM","text":"<p>We support customizable providers.</p> <p>SambaNova: <pre><code>from mini_langchain import SambaNovaLLM\n\nllm = SambaNovaLLM(\n    model=\"Meta-Llama-3.1-8B-Instruct\",\n    temperature=0.7,\n    max_tokens=200,\n    top_k=50\n)\n</code></pre></p>"},{"location":"python/getting_started/#3-chains-caching","title":"3. Chains &amp; Caching","text":"<p>Combine them into a Chain and enable caching to save money.</p> <pre><code>from mini_langchain import Chain, InMemoryCache\n\nchain = Chain(tmpl, llm)\nchain.set_cache(InMemoryCache())\n\n# First call: Costs Money (API Call)\nres = chain.invoke({\"name\": \"Alice\"})\n\n# Second call: FREE (Cached)\nres = chain.invoke({\"name\": \"Alice\"})\n</code></pre>"},{"location":"rust/api/","title":"Rust API Reference","text":"<p>While Mini_LangChain is often used via Python or Node.js, the core can also be used directly in Rust projects.</p>"},{"location":"rust/api/#crate-structure","title":"Crate Structure","text":"<ul> <li><code>mini_langchain_core</code>: The main logic.<ul> <li><code>llm</code>: Traits and implementations for providers.</li> <li><code>chain</code>: Orchestration logic.</li> <li><code>vectorstore</code>: Embedding storage and retrieval.</li> <li><code>memory</code>: Stateful session management.</li> </ul> </li> </ul>"},{"location":"rust/api/#example-usage","title":"Example Usage","text":"<pre><code>use mini_langchain_core::chain::Chain;\nuse mini_langchain_core::prompt::PromptTemplate;\nuse mini_langchain_core::providers::openai::OpenAIProvider;\n\n#[tokio::main]\nasync fn main() {\n    let prompt = PromptTemplate::new(\"Tell me about {thing}\");\n    let llm = OpenAIProvider::new(\"your-key\".to_string(), \"gpt-4\".to_string(), None, None, None, None);\n    let chain = Chain::new(prompt, Arc::new(llm));\n\n    let result = chain.invoke(HashMap::from([(\"thing\", \"Rust\")])).await;\n    println!(\"{}\", result.unwrap());\n}\n</code></pre>"},{"location":"rust/api/#internal-traits","title":"Internal Traits","text":"<p>Any new provider can be added by implementing the <code>LLM</code> trait: <pre><code>#[async_trait]\npub trait LLM: Send + Sync {\n    async fn generate(&amp;self, prompt: &amp;str) -&gt; anyhow::Result&lt;String&gt;;\n}\n</code></pre></p>"},{"location":"rust/architecture/","title":"Architecture","text":"<p>Mini_LangChain is designed for maximum throughput and minimum latency in cross-language environments.</p>"},{"location":"rust/architecture/#the-core-concept","title":"The Core Concept","text":"<p>Most LLM frameworks (like the original LangChain) are built primarily in Python. While expressive, these frameworks incur significant overhead in high-concurrency production environments due to the Global Interpreter Lock (GIL) and high-level object overhead.</p> <p>Mini_LangChain moves the entire orchestration logic\u2014prompt management, memory handling, vector similarity search, and provider communication\u2014into a single, highly optimized Rust Core.</p>"},{"location":"rust/architecture/#multi-language-bridges","title":"Multi-Language Bridges","text":"<p>We use modern, zero-overhead bridges to expose the Rust core to other ecosystems: - PyO3: For Python bindings. - Napi-rs: For Node.js bindings.</p> <pre><code>graph TD\n    A[Python App] --&gt;|FFI| B[Rust Core]\n    C[Node.js App] --&gt;|N-API| B\n    B --&gt; D[LLM Providers]\n    B --&gt; E[Vector Storage]\n    B --&gt; F[Conversation Memory]</code></pre>"},{"location":"rust/architecture/#performance-highlights","title":"Performance Highlights","text":"<ol> <li>Token Minification: Internal buffers use optimized string management to minimize allocations during prompt construction.</li> <li>Parallel Similarity Search: The <code>InMemoryVectorStore</code> leverages Rust's safe concurrency (Rayon/Tokio) for search operations.</li> <li>Zero-Copy Data: Where possible, data is passed between the bridge and the core without redundant serialization.</li> </ol>"}]}